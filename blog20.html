<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Activation Functions</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        font-family: "Poppins", sans-serif;
        box-sizing: border-box;
      }

      body {
        background: rgba(0, 0, 0, 1);
        color: rgba(255, 255, 255, 1);
      }

      nav {
        display: flex;
        align-items: center;
        justify-content: space-between;
        padding: 10px 20px;
        background-color: rgba(0, 0, 0, 0.9);
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.5);
      }

      nav .logo {
        width: 60px;
        height: 60px;
        border-radius: 50%;
        object-fit: cover;
      }

      nav ul {
        display: flex;
        list-style: none;
        text-align: left;
        padding-left: 20px;
      }

      nav ul li {
        margin: 0 10px;
      }

      nav ul li a {
        color: red;
        text-decoration: none;
        font-size: 16px;
        padding: 5px 10px;
        transition: color 0.3s;
      }

      nav ul li a:hover {
        color: white;
      }

      .image-container {
        display: flex;
        justify-content: center;
        align-items: center;
        margin: 20px auto;
        width: 90%;
        max-width: 1200px;
      }

      .image-container img {
        max-width: 50%;
        height: auto;
        border-radius: 10px;
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.5);
      }

      .container.blog-content {
        font-family: Arial, sans-serif;
        line-height: 1.6;
        color: rgba(255, 255, 255, 0.9);
        margin: 20px auto;
        width: 90%;
        max-width: 1200px;
        text-align: center;
        background: rgba(0, 0, 0, 0.95);
        padding: 30px;
        border-radius: 10px;
        box-shadow: 0px 4px 15px rgba(0, 0, 0, 0.8);
      }

      h1 {
        background-image: url("acti.png");
        background-size: cover;
        background-position: center;
        color: red;
        text-shadow: 2px 2px 5px rgba(0, 0, 0, 0.7);
        padding: 20px;
        border-radius: 10px;
        transition: color 0.3s ease-in-out;
      }

      h1:hover {
        color: white;
      }

      h2, h3 {
        color: red;
        transition: color 0.3s ease-in-out;
      }

      h2:hover, h3:hover {
        color: white;
      }

      .blog-content ul {
        text-align: left;
        padding-left: 20px;
      }

      .blog-content ul li {
        margin-bottom: 10px;
      }

      .blog-content p {
        font-size: 18px;
        line-height: 1.8;
        margin-bottom: 20px;
        text-align: justify;
        color: rgba(255, 255, 255, 0.95);
      }

      table {
        width: 100%;
        border-collapse: collapse;
        margin: 20px 0;
        background-color: rgba(0, 0, 0, 0.9);
        border-radius: 10px;
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.5);
      }

      th, td {
        padding: 15px;
        text-align: center;
        color: white;
        font-size: 16px;
        border: 1px solid rgba(255, 255, 255, 0.3);
      }

      th {
        background-color: rgba(255, 0, 0, 0.7);
        text-transform: uppercase;
      }

      tr:nth-child(even) {
        background-color: rgba(255, 255, 255, 0.1);
      }

      tr:nth-child(odd) {
        background-color: rgba(255, 255, 255, 0.2);
      }

      tr:hover {
        background-color: rgba(255, 0, 0, 0.3);
      }

      @media (max-width: 768px) {
        nav {
          flex-direction: column;
          align-items: flex-start;
        }

        nav ul {
          flex-direction: column;
          align-items: flex-start;
        }

        .image-container img {
          width: 100%;
        }
      }
    </style>
  </head>
  <body>
    <nav>
      <img src="3.png" class="logo" alt="Logo" />
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="index.html#about">About</a></li>
        <li><a href="index.html#portfolio">Portfolio</a></li>
        <li><a href="index.html#blogs">Blogs</a></li>
        <li><a href="index.html#footer">Contact</a></li>
      </ul>
    </nav>

    <div class="container blog-content">
      <h1>Activation Functions: Sigmoid, ReLU, Tanh, and their roles</h1>
      <p>Activation functions are crucial components in neural networks that introduce non-linearity into the model, allowing it to learn complex patterns and relationships in the data. Without activation functions, the neural network would essentially behave as a linear regression model, regardless of the number of layers. Here’s a brief overview of the three most commonly used activation functions: <strong>Sigmoid</strong>, <strong>ReLU (Rectified Linear Unit)</strong>, and <strong>Tanh</strong>.</p>
      
      <h2>1. Sigmoid Activation Function</h2>
      <p>The Sigmoid function is a classic activation function that maps any input to a value between 0 and 1. It is commonly used in binary classification problems because it outputs probabilities.</p>
      
      <h3>Formula:</h3>
      <pre><code>Sigmoid(x) = 1 / (1 + e^(-x))</code></pre>
      
      <p><strong>Range:</strong> (0, 1)</p>
      <p><strong>Shape:</strong> S-shaped curve</p>
      <p><strong>Key Property:</strong> It squashes inputs to a small range, making it suitable for output layers of binary classifiers.</p>

      <h3>Pros:</h3>
      <ul>
        <li>Maps outputs to a probability range (0, 1), useful in the final layer of a classifier.</li>
        <li>Easy to differentiate, making it useful for backpropagation.</li>
      </ul>

      <h3>Cons:</h3>
      <ul>
        <li><strong>Vanishing Gradient Problem:</strong> When inputs are too large or too small, gradients become very small, leading to slow learning or no learning at all during training.</li>
        <li>Can saturate at 0 or 1, causing ineffective weight updates.</li>
      </ul>

      <h3>Usage:</h3>
      <p>Often used in binary classification tasks, especially when the output represents a probability (e.g., in logistic regression).</p>

      <hr>

      <h2>2. ReLU (Rectified Linear Unit) Activation Function</h2>
      <p>ReLU is one of the most popular activation functions in modern deep learning models. It outputs the input directly if it’s positive, otherwise, it outputs zero.</p>

      <h3>Formula:</h3>
      <pre><code>ReLU(x) = max(0, x)</code></pre>

      <p><strong>Range:</strong> [0, ∞)</p>
      <p><strong>Shape:</strong> A piecewise linear function</p>
      <p><strong>Key Property:</strong> Introduces sparsity in the network (i.e., many outputs are zero), helping the network focus on important features.</p>

      <h3>Pros:</h3>
      <ul>
        <li><strong>Non-saturating:</strong> Unlike Sigmoid, ReLU does not saturate for positive inputs, helping to mitigate the vanishing gradient problem.</li>
        <li><strong>Computationally efficient:</strong> Simple to compute and often results in faster convergence.</li>
      </ul>

      <h3>Cons:</h3>
      <ul>
        <li><strong>Dying ReLU Problem:</strong> If the weights are initialized poorly, many neurons can become inactive during training and stop updating (outputting zero for all inputs).</li>
      </ul>

      <h3>Usage:</h3>
      <p>Commonly used in the hidden layers of neural networks for most machine learning tasks.</p>

      <hr>

      <h2>3. Tanh (Hyperbolic Tangent) Activation Function</h2>
      <p>The Tanh function is another smooth activation function that maps input values to a range between -1 and 1. It’s similar to the Sigmoid function but has a wider output range.</p>

      <h3>Formula:</h3>
      <pre><code>Tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code></pre>

      <p><strong>Range:</strong> (-1, 1)</p>
      <p><strong>Shape:</strong> S-shaped curve, but centered around zero.</p>
      <p><strong>Key Property:</strong> The output is zero-centered, which helps to mitigate issues with optimization because the gradients are balanced around zero.</p>

      <h3>Pros:</h3>
      <ul>
        <li>Zero-centered output, which can make optimization faster and more stable.</li>
        <li>Avoids the problem of outputs being bounded within the positive range (like Sigmoid).</li>
      </ul>

      <h3>Cons:</h3>
      <ul>
        <li><strong>Vanishing Gradient Problem:</strong> Similar to the Sigmoid function, the gradients can become very small for large positive or negative inputs, slowing down training.</li>
        <li>More computationally expensive than ReLU.</li>
      </ul>

      <h3>Usage:</h3>
      <p>Often used in RNNs (Recurrent Neural Networks) for tasks like language modeling and speech recognition because of its zero-centered output.</p>

      <hr>

      <h2>Roles of Activation Functions</h2>
      <ul>
        <li><strong>Introducing Non-linearity:</strong> Neural networks need to learn complex patterns, and linear functions (without activation) would limit their expressiveness. Activation functions help introduce the necessary non-linearity.</li>
        <li><strong>Improving Learning:</strong> Activation functions enable backpropagation by providing gradients that can be used to update weights.</li>
        <li><strong>Preventing Exploding or Vanishing Gradients:</strong> Functions like ReLU help reduce the vanishing gradient problem, allowing deeper networks to train more efficiently.</li>
        <li><strong>Different Use Cases:</strong> Each activation function has different strengths for various tasks.</li>
      </ul>
    </div>
  </body>
</html>
